{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giacomoarienti/data-intensive-lab/blob/master/sae-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0Kh5x9Z7B5V_",
        "outputId": "5d74842c-9937-4a5c-cedd-5d75fd42f521",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "8gROfSCsB5WE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple\n",
        "\n",
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim: int, expansion_factor: float = 16):\n",
        "        super().__init__()\n",
        "        # n -> n * factor -> n\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = int(input_dim * expansion_factor)\n",
        "        self.decoder = nn.Linear(self.latent_dim, input_dim, bias=True)\n",
        "        self.encoder = nn.Linear(input_dim, self.latent_dim, bias=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        encoded = F.relu(self.encoder(x))\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded, encoded\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            return F.relu(self.encoder(x))\n",
        "\n",
        "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            return self.decoder(x)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, path: str, input_dim: int, expansion_factor: float = 16, device: str = \"cuda\") -> \"SparseAutoencoder\":\n",
        "        model = cls(input_dim=input_dim, expansion_factor=expansion_factor)\n",
        "        state_dict = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "6LCX1dXMB5WG"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "jo30HqC6B5WH"
      },
      "outputs": [],
      "source": [
        "# sae_name = \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
        "sae_name = \"Llama-3.2-1B-Instruct-SAE-l9\"\n",
        "# sae_name = \"DeepSeek-R1-Distill-Llama-70B-SAE-l48\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "erYZ_E2vB5WI"
      },
      "outputs": [],
      "source": [
        "file_path = hf_hub_download(\n",
        "    repo_id=f\"qresearch/{sae_name}\",\n",
        "    filename=f\"{sae_name}.pt\",\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eJ-ZbNujB5WI",
        "outputId": "3e7c5eee-fc84-4071-8da3-e256087705a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-0fec132e3e66>:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(path, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = (\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" if sae_name == \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
        "              else \"meta-llama/Llama-3.2-1B-Instruct\" if sae_name == \"Llama-3.2-1B-Instruct-SAE-l9\"\n",
        "              else \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"bfloat16\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "expansion_factor = 8 if sae_name == \"DeepSeek-R1-Distill-Llama-70B-SAE-l48\" else 16\n",
        "sae = SparseAutoencoder.from_pretrained(\n",
        "    path=file_path,\n",
        "    input_dim=model.config.hidden_size,\n",
        "    expansion_factor=expansion_factor,\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "F0ceUu8KB5WJ",
        "outputId": "d60f8652-fc77-4ddb-9d32-2a334d0585cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 13 Mar 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Hello, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I'm doing well, thanks for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm here to help you with any questions or topics you'd like to discuss. How about\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
        "    ],\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "cYKVaY1EB5WK"
      },
      "outputs": [],
      "source": [
        "def gather_residual_activations(model, target_layer, inputs):\n",
        "    target_act = None\n",
        "    def gather_target_act_hook(mod, inputs, outputs):\n",
        "        nonlocal target_act\n",
        "        target_act = inputs[0]  # Get residual stream from layer input\n",
        "        return outputs\n",
        "\n",
        "    handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n",
        "    with torch.no_grad():\n",
        "        _ = model(inputs)\n",
        "    handle.remove()\n",
        "    return target_act"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show model architecture\n",
        "print(model.model.layers)"
      ],
      "metadata": {
        "id": "GahjuiRzKivj",
        "outputId": "cf4cff7b-226b-47aa-affa-300a948f1ebd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModuleList(\n",
            "  (0-15): 16 x LlamaDecoderLayer(\n",
            "    (self_attn): LlamaAttention(\n",
            "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "    )\n",
            "    (mlp): LlamaMLP(\n",
            "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "      (act_fn): SiLU()\n",
            "    )\n",
            "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8_8EQ5BYB5WK"
      },
      "outputs": [],
      "source": [
        "layer_id = (19 if sae_name == \"DeepSeek-R1-Distill-Llama-8B-SAE-l19\"\n",
        "            else 9 if sae_name == \"Llama-3.2-1B-Instruct-SAE-l9\"\n",
        "            else 48)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DtHLltqEB5WL"
      },
      "outputs": [],
      "source": [
        "target_act = gather_residual_activations(model, layer_id, inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "HvvEAsjuB5WL"
      },
      "outputs": [],
      "source": [
        "def ensure_same_device(sae, target_act):\n",
        "    \"\"\"Ensure SAE and activations are on the same device\"\"\"\n",
        "    model_device = target_act.device\n",
        "    sae = sae.to(model_device)\n",
        "    return sae, target_act.to(model_device)\n",
        "\n",
        "sae, target_act = ensure_same_device(sae, target_act)\n",
        "sae_acts = sae.encode(target_act.to(torch.float32))\n",
        "recon = sae.decode(sae_acts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "1HA0hXDMB5WL",
        "outputId": "b9ba2787-04c4-4675-a037-9f32eae89040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance explained: 0.999\n"
          ]
        }
      ],
      "source": [
        "var_explained = 1 - torch.mean((recon - target_act.to(torch.float32)) ** 2) / torch.var(target_act.to(torch.float32))\n",
        "print(f\"Variance explained: {var_explained:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "B4-wDfcXB5WM"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Golden Gate Bridge\"},\n",
        "    ],\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_yUqEOmB5WM",
        "outputId": "82789860-1da7-4888-cf45-ba02441acbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top activated features during pirate speech:\n",
            "Feature 25317: 0.405\n",
            "Feature 30078: 0.293\n",
            "Feature 28128: 0.237\n",
            "Feature 9942: 0.219\n",
            "Feature 13916: 0.189\n",
            "Feature 3663: 0.156\n",
            "Feature 8103: 0.143\n",
            "Feature 31638: 0.140\n",
            "Feature 5833: 0.138\n",
            "Feature 2052: 0.134\n",
            "Feature 28447: 0.116\n",
            "Feature 15786: 0.108\n",
            "Feature 28539: 0.107\n",
            "Feature 31021: 0.099\n",
            "Feature 22263: 0.092\n",
            "Feature 16742: 0.079\n",
            "Feature 30504: 0.077\n",
            "Feature 25757: 0.075\n",
            "Feature 18508: 0.074\n",
            "Feature 11775: 0.072\n",
            "\n",
            "Activation patterns across tokens:\n",
            "\n",
            "Token: ĊĊ\n",
            "  Feature 28539: 0.536\n",
            "  Feature 31021: 0.497\n",
            "  Feature 11775: 0.362\n",
            "\n",
            "Token: Golden\n",
            "  Feature 30078: 1.465\n",
            "  Feature 3663: 0.497\n",
            "  Feature 16742: 0.396\n",
            "\n",
            "Token: ĠGate\n",
            "  Feature 25317: 0.596\n",
            "  Feature 28128: 0.274\n",
            "  Feature 8103: 0.713\n",
            "  Feature 31638: 0.246\n",
            "  Feature 5833: 0.688\n",
            "  Feature 15786: 0.375\n",
            "  Feature 18508: 0.370\n",
            "\n",
            "Token: ĠBridge\n",
            "  Feature 25317: 1.242\n",
            "  Feature 28128: 0.730\n",
            "  Feature 9942: 0.742\n",
            "  Feature 13916: 0.500\n",
            "  Feature 31638: 0.309\n",
            "  Feature 2052: 0.354\n",
            "  Feature 28447: 0.580\n",
            "  Feature 22263: 0.459\n",
            "  Feature 25757: 0.216\n",
            "\n",
            "Token: <|eot_id|>\n",
            "  Feature 9942: 0.352\n",
            "  Feature 13916: 0.360\n",
            "  Feature 2052: 0.314\n",
            "  Feature 30504: 0.383\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Get activations\n",
        "target_act = gather_residual_activations(model, layer_id, inputs)\n",
        "sae_acts = sae.encode(target_act.to(torch.float32))\n",
        "\n",
        "# Get token IDs and decode them for reference\n",
        "tokens = inputs[0].cpu().numpy()\n",
        "token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
        "\n",
        "# Find which tokens are part of the assistant's response\n",
        "token_ids = inputs[0].cpu().numpy()\n",
        "is_special = (token_ids >= 128000) & (token_ids <= 128255)\n",
        "special_positions = np.where(is_special)[0]\n",
        "\n",
        "assistant_start = special_positions[-2] + 1\n",
        "assistant_tokens = slice(assistant_start, None)\n",
        "\n",
        "# Get activation statistics for assistant's response\n",
        "assistant_activations = sae_acts[0, assistant_tokens]\n",
        "mean_activations = assistant_activations.mean(dim=0)\n",
        "\n",
        "# Find top activated features during pirate speech\n",
        "num_top_features = 20\n",
        "top_features = mean_activations.topk(num_top_features)\n",
        "\n",
        "print(\"Top activated features during pirate speech:\")\n",
        "for idx, value in zip(top_features.indices, top_features.values):\n",
        "    print(f\"Feature {idx}: {value:.3f}\")\n",
        "\n",
        "# Look at how these features activate across different tokens\n",
        "print(\"\\nActivation patterns across tokens:\")\n",
        "for i, (token, acts) in enumerate(zip(token_texts[assistant_tokens], assistant_activations)):\n",
        "    top_acts = acts[top_features.indices]\n",
        "    if top_acts.max() > 0.2:  # Only show tokens with significant activation\n",
        "        print(f\"\\nToken: {token}\")\n",
        "        for feat_idx, act_val in zip(top_features.indices, top_acts):\n",
        "            if act_val > 0.2:  # Threshold for \"active\" features\n",
        "                print(f\"  Feature {feat_idx}: {act_val:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "E6MLH9ooB5WN"
      },
      "outputs": [],
      "source": [
        "def generate_with_intervention(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    sae,\n",
        "    messages: list[dict],\n",
        "    feature_idx: list[int],\n",
        "    intervention: float = 3.0,\n",
        "    target_layer: int = 9,\n",
        "    max_new_tokens: int = 50\n",
        "):\n",
        "    modified_activations = None\n",
        "\n",
        "    def intervention_hook(module, inputs, outputs):\n",
        "        nonlocal modified_activations\n",
        "        activations = inputs[0]\n",
        "\n",
        "        features = sae.encode(activations.to(torch.float32))\n",
        "        reconstructed = sae.decode(features)\n",
        "        error = activations.to(torch.float32) - reconstructed\n",
        "\n",
        "        features[:, :, feature_idx] += intervention\n",
        "\n",
        "        modified = sae.decode(features) + error\n",
        "        modified_activations = modified\n",
        "        modified_activations = modified.to(torch.bfloat16)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def output_hook(module, inputs, outputs):\n",
        "        nonlocal modified_activations\n",
        "        if modified_activations is not None:\n",
        "            return (modified_activations,) + outputs[1:] if len(outputs) > 1 else (modified_activations,)\n",
        "        return outputs\n",
        "\n",
        "    handles = [\n",
        "        model.model.layers[target_layer].register_forward_hook(intervention_hook),\n",
        "        model.model.layers[target_layer].register_forward_hook(output_hook)\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        input_tokens = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_tokens,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False  # Use greedy decoding for consistency\n",
        "        )\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "    finally:\n",
        "        for handle in handles:\n",
        "            handle.remove()\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How are you doing?\"}\n",
        "    # {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
        "    # {\"role\": \"user\", \"content\": \"Roleplay as a pirate\"}\n",
        "]\n",
        "features_to_modify = [25317, 30078, 28128]\n",
        "intervation = 1.5\n",
        "\n",
        "print(\"Original generation:\")\n",
        "input_tokens = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "outputs = model.generate(input_tokens, max_new_tokens=1000, do_sample=False)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "print(\"\\nGeneration with modified feature:\")\n",
        "modified_text = generate_with_intervention(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    sae=sae,\n",
        "    messages=messages,\n",
        "    feature_idx=features_to_modify,\n",
        "    intervention=intervation,\n",
        "    target_layer=layer_id,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "print(modified_text)"
      ],
      "metadata": {
        "id": "yXYnosHcOJpS",
        "outputId": "89aab6eb-f19b-4c09-dc36-48a64ea320ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original generation:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 13 Mar 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How are you doing?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I'm doing well, thanks for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm always ready to help with any questions or topics you'd like to discuss. How about you? How's your day going?<|eot_id|>\n",
            "\n",
            "Generation with modified feature:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 13 Mar 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How are you doing?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "This is a replica of the statue of the sun at the Eiffel Tower.<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3EIgtOteB5WO",
        "outputId": "2ba3d17a-65ba-4e3d-94d7-24c8424a614f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generation with modified feature:\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 13 Mar 2025\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How many Rs in strawberry?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The number of Rs in \"Strawberry\" is 2.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nGeneration with modified feature:\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How many Rs in strawberry?\"}\n",
        "]\n",
        "\n",
        "modified_text = generate_with_intervention(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    sae=sae,\n",
        "    messages=messages,\n",
        "    feature_idx=features_to_modify,\n",
        "    intervention=8,\n",
        "    target_layer=layer_id,\n",
        "    max_new_tokens=1000\n",
        ")\n",
        "print(modified_text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}